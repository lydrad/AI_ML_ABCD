{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m; shap\u001b[38;5;241m.\u001b[39minitjs()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_classification \n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, KFold, cross_validate\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "\n",
    "import shap; shap.initjs()\n",
    "\n",
    "from sklearn.datasets import make_classification \n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from importlib import reload\n",
    "import week_5_helperFx as fx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I realize that I don't have shap downloaded and I can't run many of the analyses for this exercise. I have tried troubleshooting it, but it will not download either with pip or conda. I will go to IT support to fix this, but in the mean time, here are my results. For interpretation, I looked at the github answers to see the SHAP plots. Thanks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code to get train/test accuracy, how does the model do?\n",
    "After you run this,\n",
    "add code, where specified, to create a test sets that are stratified by sex,\n",
    "uncomment associated lines to store and print accuracy stratified by sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'col_names.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m num_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_iter)):\n\u001b[1;32m     13\u001b[0m     \n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Generate synthetic data\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     dat \u001b[38;5;241m=\u001b[39m \u001b[43mfx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreate_MDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_n\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m dat\u001b[38;5;241m.\u001b[39mX, dat\u001b[38;5;241m.\u001b[39my\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Create splits\u001b[39;00m\n",
      "File \u001b[0;32m~/DS_Training/week_5_helperFx.py:36\u001b[0m, in \u001b[0;36mCreate_MDD.__init__\u001b[0;34m(self, n_m, n_f)\u001b[0m\n\u001b[1;32m     33\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data \u001b[38;5;241m=\u001b[39m X)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Get column names\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcol_names.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     37\u001b[0m     colnames \u001b[38;5;241m=\u001b[39m [nm\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m nm \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines()]\n\u001b[1;32m     39\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m colnames\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'col_names.txt'"
     ]
    }
   ],
   "source": [
    "# Number of males/females in our dataset\n",
    "m_n = 10000\n",
    "f_n = 500\n",
    "\n",
    "# Store performance\n",
    "tr_acc, te_acc, m_te_acc, f_te_acc = [], [], [], []\n",
    "\n",
    "\n",
    "# Train a couple times (synthetic data performance can vary)\n",
    "num_iter = 10\n",
    "\n",
    "for ii in tqdm(range(num_iter)):\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    dat = fx.Create_MDD(m_n, f_n)\n",
    "    X, y = dat.X, dat.y\n",
    "\n",
    "    # Create splits\n",
    "    kf = KFold(n_splits=5)\n",
    "    \n",
    "    # Train/eval for each fold\n",
    "    for tr_idx, te_idx in kf.split(y):\n",
    "        \n",
    "        # Split train/test\n",
    "        tr_x, te_x =  X.iloc[tr_idx, :], X.iloc[te_idx, :]\n",
    "        tr_y, te_y =  y.iloc[tr_idx], y.iloc[te_idx]\n",
    "\n",
    "        # Fit our model\n",
    "        logl1 = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "        logl1.fit(tr_x, tr_y)\n",
    "                \n",
    "        # Get our predictions\n",
    "        tr_pred = logl1.predict(tr_x)\n",
    "        te_pred = logl1.predict(te_x)\n",
    "        \n",
    "        # Get accuracy\n",
    "        tr_acc.append(accuracy_score(tr_y, tr_pred))\n",
    "        te_acc.append(accuracy_score(te_y, te_pred))\n",
    "        \n",
    "        # Stratify test set by sex\n",
    "        m_te_x = te_x[te_x['sex'] == 0]\n",
    "        m_te_y = te_y[te_x['sex'] == 0]\n",
    "        f_te_x = te_x[te_x['sex'] == 1]\n",
    "        f_te_y = te_y[te_x['sex'] == 1]\n",
    "\n",
    "        # Get our predictions\n",
    "        te_pred_m = logl1.predict(m_te_x)\n",
    "        te_pred_f = logl1.predict(f_te_x)\n",
    "\n",
    "        # Get accuracy\n",
    "        m_te_acc.append(accuracy_score(m_te_y, te_pred_m))\n",
    "        f_te_acc.append(accuracy_score(f_te_y, te_pred_f))\n",
    "\n",
    "\n",
    "print(f'Train Acc: {np.mean(np.array(tr_acc)):.3f} '\n",
    "      f'Test Acc: {np.mean(np.array(te_acc)):.3f} '\n",
    "      f'Test Acc Male: {np.mean(np.array(m_te_acc)):.3f} '\n",
    "      f'Test Acc Female: {np.mean(np.array(f_te_acc)):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code to get train/test accuracy, how does the model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.808 Test Acc: 0.806\n"
     ]
    }
   ],
   "source": [
    "# Generate the data from the helper fx\n",
    "dataset = fx.Create_GAD()\n",
    "dataset.to_dframe()\n",
    "\n",
    "# Store performance\n",
    "tr_acc, te_acc = [], []\n",
    "\n",
    "# Split X, y\n",
    "y = dataset.df['g_anxd']\n",
    "X = dataset.df.drop(['g_anxd', 'x'], axis = 1)\n",
    "\n",
    "# Create splits\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Train/eval for each fold\n",
    "for tr_idx, te_idx in kf.split(y):\n",
    "    \n",
    "    # Split train/test\n",
    "    tr_x, te_x =  X.iloc[tr_idx, :], X.iloc[te_idx, :]\n",
    "    tr_y, te_y =  y.iloc[tr_idx], y.iloc[te_idx]\n",
    "\n",
    "    # Fit our model\n",
    "    logl1 = LogisticRegression(solver='liblinear')# penalty='l1', \n",
    "    logl1.fit(tr_x, tr_y)\n",
    "\n",
    "    # Get our predictions\n",
    "    tr_pred = logl1.predict(tr_x)\n",
    "    te_pred = logl1.predict(te_x)\n",
    "\n",
    "    tr_p = logl1.predict_log_proba(tr_x)\n",
    "    tr_logg_odds = tr_p[:,1] - tr_p[:,0]\n",
    "    \n",
    "    # Get accuracy\n",
    "    tr_acc.append(accuracy_score(tr_y, tr_pred))\n",
    "    te_acc.append(accuracy_score(te_y, te_pred))\n",
    "\n",
    "print(f'Train Acc: {np.mean(np.array(tr_acc)):.3f} Test Acc: {np.mean(np.array(te_acc)):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How does our performance vary as a function of composition of M/F?\n",
    "\n",
    "- The accuracy dips for Males\n",
    "\n",
    "2. Is this something we would have caught just viewing total accuracy?\n",
    "\n",
    "- No, not really. The total accuracy does dip slightly, but not much.\n",
    "\n",
    "3. Might this create health disparities? How?\n",
    "\n",
    "- Yes, if we are making predictions based on disparaged data and generalizing them to the whole, this would create health disparities. Males may not get the same level of accurate predictions/treatment as females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'col_names.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Train a couple times (synthetic data performance can vary)=\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Generate data\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     dat \u001b[38;5;241m=\u001b[39m \u001b[43mfx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreate_MDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_n\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m dat\u001b[38;5;241m.\u001b[39mX, dat\u001b[38;5;241m.\u001b[39my\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Create splits\u001b[39;00m\n",
      "File \u001b[0;32m~/DS_Training/week_5_helperFx.py:36\u001b[0m, in \u001b[0;36mCreate_MDD.__init__\u001b[0;34m(self, n_m, n_f)\u001b[0m\n\u001b[1;32m     33\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data \u001b[38;5;241m=\u001b[39m X)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Get column names\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcol_names.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     37\u001b[0m     colnames \u001b[38;5;241m=\u001b[39m [nm\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m nm \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines()]\n\u001b[1;32m     39\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m colnames\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'col_names.txt'"
     ]
    }
   ],
   "source": [
    "# Range of splits we'll try\n",
    "x = np.arange(.05, .75, .025)\n",
    "\n",
    "# Total number of subjects\n",
    "n = 2000\n",
    "\n",
    "# Create overarching arrays to store performance\n",
    "perf = np.zeros(x.shape)\n",
    "m_perf = np.zeros(x.shape)\n",
    "f_perf = np.zeros(x.shape)\n",
    "\n",
    "# Loop through all ranges of x\n",
    "for ind, p in enumerate(tqdm(x)):\n",
    "    \n",
    "    # Composition of dataset\n",
    "    m_n = int(n * (1 - p))\n",
    "    f_n = int(n * p)\n",
    "\n",
    "    # Store performance\n",
    "    te_acc, m_te_acc, f_te_acc = [], [], []\n",
    "\n",
    "    # Train a couple times (synthetic data performance can vary)=\n",
    "    for ii in range(20):\n",
    "\n",
    "        # Generate data\n",
    "        dat = fx.Create_MDD(m_n, f_n)\n",
    "        X, y = dat.X, dat.y\n",
    "\n",
    "        # Create splits\n",
    "        kf = KFold(n_splits=5)\n",
    "\n",
    "        # Train/eval for each fold\n",
    "        for tr_idx, te_idx in kf.split(y):\n",
    "            \n",
    "            # Split train/test\n",
    "            tr_x, te_x =  X.iloc[tr_idx, :], X.iloc[te_idx, :]\n",
    "            tr_y, te_y =  y.iloc[tr_idx], y.iloc[te_idx]\n",
    "\n",
    "            # Fit our model\n",
    "            logl1 = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "            logl1.fit(tr_x, tr_y)\n",
    "\n",
    "            # Get our predictions\n",
    "            te_pred = logl1.predict(te_x)\n",
    "\n",
    "            # Get accuracy\n",
    "            te_acc.append(accuracy_score(te_y, te_pred))\n",
    "\n",
    "            \n",
    "            '''#################################################\n",
    "            # Split test into M/F only and evaluate performance\n",
    "            #################################################'''\n",
    "\n",
    "            # Stratify test set by sex\n",
    "            m_te_x = te_x[te_x['sex'] == 0]\n",
    "            m_te_y = te_y[te_x['sex'] == 0]\n",
    "            f_te_x = te_x[te_x['sex'] == 1]\n",
    "            f_te_y = te_y[te_x['sex'] == 1]\n",
    "\n",
    "            # Get our predictions\n",
    "            te_pred_m = logl1.predict(m_te_x)\n",
    "            te_pred_f = logl1.predict(f_te_x)\n",
    "            \n",
    "            m_te_acc.append(accuracy_score(m_te_y, te_pred_m))\n",
    "            f_te_acc.append(accuracy_score(f_te_y, te_pred_f))\n",
    "\n",
    "    # Store performance\n",
    "    perf[ind] = np.mean(np.array(te_acc))\n",
    "    m_perf[ind] = np.mean(np.array(m_te_acc))\n",
    "    f_perf[ind] = np.mean(np.array(f_te_acc))\n",
    "\n",
    "    \n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize = (12, 8))\n",
    "\n",
    "# For each model\n",
    "ax.plot(x, perf, color = 'green', label = 'Total Acc')\n",
    "ax.plot(x, m_perf, color = 'red', label = 'M Acc')\n",
    "ax.plot(x, f_perf, color = 'blue', label = 'F Acc')\n",
    "ax.scatter(x, perf, color = 'green')\n",
    "ax.scatter(x, m_perf, color = 'red')\n",
    "ax.scatter(x, f_perf, color = 'blue')\n",
    "\n",
    "ax.set_title('Underrepresentation')\n",
    "ax.set_xlabel('Percent F')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code to get train/test accuracy, how does the model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.821 Test Acc: 0.814\n"
     ]
    }
   ],
   "source": [
    "# Generate the data from the helper fx\n",
    "dataset = fx.Create_GAD()\n",
    "dataset.to_dframe()\n",
    "\n",
    "# Store performance\n",
    "tr_acc, te_acc = [], []\n",
    "\n",
    "# Split X, y\n",
    "y = dataset.df['g_anxd']\n",
    "X = dataset.df.drop(['g_anxd', 'x'], axis = 1)\n",
    "\n",
    "# Create splits\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Train/eval for each fold\n",
    "for tr_idx, te_idx in kf.split(y):\n",
    "    \n",
    "    # Split train/test\n",
    "    tr_x, te_x =  X.iloc[tr_idx, :], X.iloc[te_idx, :]\n",
    "    tr_y, te_y =  y.iloc[tr_idx], y.iloc[te_idx]\n",
    "\n",
    "    # Fit our model\n",
    "    logl1 = LogisticRegression(solver='liblinear')# penalty='l1', \n",
    "    logl1.fit(tr_x, tr_y)\n",
    "\n",
    "    # Get our predictions\n",
    "    tr_pred = logl1.predict(tr_x)\n",
    "    te_pred = logl1.predict(te_x)\n",
    "\n",
    "    tr_p = logl1.predict_log_proba(tr_x)\n",
    "    tr_logg_odds = tr_p[:,1] - tr_p[:,0]\n",
    "    \n",
    "    # Get accuracy\n",
    "    tr_acc.append(accuracy_score(tr_y, tr_pred))\n",
    "    te_acc.append(accuracy_score(te_y, te_pred))\n",
    "\n",
    "print(f'Train Acc: {np.mean(np.array(tr_acc)):.3f} Test Acc: {np.mean(np.array(te_acc)):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not have great accuracy: 0.821 for training and 0.814 for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.991 Test Acc: 0.987\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of our dataset\n",
    "dataset = fx.Create_GAD()\n",
    "\n",
    "# Add the context variables\n",
    "dataset.add_context()\n",
    "\n",
    "# Store performance\n",
    "tr_acc, te_acc = [], []\n",
    "\n",
    "# Split X,y\n",
    "y = dataset.df['g_anxd']\n",
    "X = dataset.df.drop(['g_anxd', 'x'], axis = 1)\n",
    "\n",
    "# Create splits\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Train/eval for each fold\n",
    "for tr_idx, te_idx in kf.split(y):\n",
    "    \n",
    "    # Split train/test\n",
    "    tr_x, te_x =  X.iloc[tr_idx, :], X.iloc[te_idx, :]\n",
    "    tr_y, te_y =  y.iloc[tr_idx], y.iloc[te_idx]\n",
    "\n",
    "    # Fit our model\n",
    "    logl1 = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "    logl1.fit(tr_x, tr_y)\n",
    "\n",
    "    # Get our predictions\n",
    "    tr_pred = logl1.predict(tr_x)\n",
    "    te_pred = logl1.predict(te_x)\n",
    "\n",
    "    # Get accuracy\n",
    "    tr_acc.append(accuracy_score(tr_y, tr_pred))\n",
    "    te_acc.append(accuracy_score(te_y, te_pred))\n",
    "\n",
    "print(f'Train Acc: {np.mean(np.array(tr_acc)):.3f} Test Acc: {np.mean(np.array(te_acc)):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the limitations of SHAP?\n",
    "\n",
    "[x] Calculating the SHAP value for large datasets requires a large amount of computational resources\n",
    "\n",
    "[x] Potential misinterpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create the explainer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241m.\u001b[39mLinearExplainer(logl1, tr_x)\n\u001b[1;32m      3\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer(tr_x)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Plot\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shap' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the explainer\n",
    "explainer = shap.LinearExplainer(logl1, tr_x)\n",
    "shap_values = explainer(tr_x)\n",
    "\n",
    "# Plot\n",
    "shap.plots.bar(shap_values)\n",
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which features now emerge as most important?\n",
    "\n",
    "- Food insecurity, neighborhood violence, and access to health care are more important than income\n",
    "\n",
    "Why include these elements of social deteriminants of health?\n",
    "\n",
    "- These are informative features as they possibly inform legislation that is needed to protect health of US citizens. They explain risk for depression better than income does.\n",
    "\n",
    "Using the ABCD data dictionary identify 5-10 features which reflect social deteriminants of health\n",
    "\n",
    "- ABCD child nutrition assessment, ABCD Longitudinal Parent Medical History Questionnaire, ABCD Youth Block Food Screen, ABCD Sum Scores Physical Health Youth, ABCD School Risk and Protective Factors Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.986 Test Acc: 0.984\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of our dataset\n",
    "dataset = fx.Create_GAD()\n",
    "\n",
    "# Add the context variables\n",
    "dataset.add_context()\n",
    "\n",
    "# Store performance\n",
    "tr_acc, te_acc = [], []\n",
    "\n",
    "# Split X,y\n",
    "y = dataset.df['g_anxd']\n",
    "X = dataset.df.drop(['g_anxd', 'x'], axis = 1)\n",
    "\n",
    "# Create splits\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Train/eval for each fold\n",
    "for tr_idx, te_idx in kf.split(y):\n",
    "    \n",
    "    # Split train/test\n",
    "    tr_x, te_x =  X.iloc[tr_idx, :], X.iloc[te_idx, :]\n",
    "    tr_y, te_y =  y.iloc[tr_idx], y.iloc[te_idx]\n",
    "\n",
    "    # Fit our model\n",
    "    logl1 = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "    logl1.fit(tr_x, tr_y)\n",
    "\n",
    "    # Get our predictions\n",
    "    tr_pred = logl1.predict(tr_x)\n",
    "    te_pred = logl1.predict(te_x)\n",
    "\n",
    "    # Get accuracy\n",
    "    tr_acc.append(accuracy_score(tr_y, tr_pred))\n",
    "    te_acc.append(accuracy_score(te_y, te_pred))\n",
    "\n",
    "print(f'Train Acc: {np.mean(np.array(tr_acc)):.3f} Test Acc: {np.mean(np.array(te_acc)):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create the explainer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241m.\u001b[39mLinearExplainer(logl1, tr_x)\n\u001b[1;32m      3\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer(tr_x)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Plot\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shap' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the explainer\n",
    "explainer = shap.LinearExplainer(logl1, tr_x)\n",
    "shap_values = explainer(tr_x)\n",
    "\n",
    "# Plot\n",
    "shap.plots.bar(shap_values.abs.mean(0))\n",
    "shap.plots.beeswarm(shap_values)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
